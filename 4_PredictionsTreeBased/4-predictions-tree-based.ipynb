{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO-F-422 -  Statistical Foundations of Machine Learning \n",
    "\n",
    "### Bertrand Lebichot - __[Bertrand.Lebichot@ulb.ac.be](mailto:Bertrand.Lebichot@ulb.ac.be)__\n",
    "### Jacopo De Stefani - __[Jacopo.De.Stefani@ulb.ac.be](mailto:Jacopo.De.Stefani@ulb.ac.be)__\n",
    "### Arnaud Pollaris - __[Arnaud.Pollaris@ulb.ac.be](mailto:Arnaud.Pollaris@ulb.ac.be)__\n",
    "### Gianluca Bontempi - __[gbonte@ulb.ac.be](mailto:gbonte@ulb.ac.be)__\n",
    "\n",
    "## TP 4 - Predictions: Tree-based methods\n",
    "\n",
    "####  April 02, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal and dataset\n",
    "\n",
    "The goal is to review some ML concepts :\n",
    "* Classification trees\n",
    "* Ensemble methods : Random forests\n",
    "* Ensemble methods : Adaboost\n",
    "\n",
    "The chosen database is about spam detection. Details can be found here :\n",
    "https://archive.ics.uci.edu/ml/datasets/spambase\n",
    "\n",
    "To import the dataset, just use package ''nutshell'' (uncomment install line if required).\n",
    "\n",
    "Never forget that reading the documentation is useful :\n",
    "*? name_of_function* or *help(name_of_function)* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"nutshell\", repos = \"http://cran.rstudio.com\")\n",
    "library(\"nutshell\")\n",
    "data(spambase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification vs regression\n",
    "\n",
    "Both classification and regression are sub-fields of *supervised learning*. In the two cases, we have predictive variables $X$ and a target variable $y$. The learning is said to be supervised because we use the actual value of $y$ for each samples in the training process.\n",
    "\n",
    "In regression, $y$ is assumed to be a continuous variable. In classification however, $y$ is assumed to be a discrete variable.\n",
    "\n",
    "The goal of a classification task is to automatically assign data to predefined discrete\n",
    "classes (where $C$ is the number of classes). \n",
    "\n",
    "Here is a toy example :\n",
    "* Knowing the two abstract features F1 and F2, and six samples for both class (the red class and the blue class), can you predict the label for the green sample ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAOVBMVEUAAAAAAP8A/wBNTU1o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\nAAGEBAj4Hyth8lAFwZ3XAAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"Scatterplot and decision boundary\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 6\n",
    "\n",
    "# blue class data\n",
    "F1 = rnorm(n, mean = +2, sd = 1)\n",
    "F2 = rnorm(n, mean = +1, sd = 1)\n",
    "SamplesBLUE = data.frame(F1, F2)\n",
    "gBLUE = c(mean(F1),mean(F2))\n",
    "\n",
    "# red class data\n",
    "F1 = rnorm(n, mean = -2, sd = 1)\n",
    "F2 = rnorm(n, mean = -1, sd = 1)\n",
    "SamplesRED = data.frame(F1, F2)\n",
    "gRED = c(mean(F1),mean(F2))\n",
    "\n",
    "# draw scatterplot\n",
    "plot(SamplesBLUE$F1,SamplesBLUE$F2,col=\"blue\",main=\"Scatterplot and decision boundary\",xlab=\"F1\",ylab=\"F2\",xlim=c(-5,5),ylim=c(-5,5))\n",
    "points(SamplesRED$F1,SamplesRED$F2,col=\"red\")\n",
    "points(0,0,col=\"green\")\n",
    "\n",
    "# draw centroid of classes\n",
    "points(gBLUE[1],gBLUE[2],col=\"blue\",pch = 4)\n",
    "points(gRED[1],gRED[2],col=\"red\",pch = 4)\n",
    "\n",
    "# draw decision boundary\n",
    "midgg = c(gRED[1]+abs(gBLUE[1]-gRED[1])/2,gRED[2]+abs(gBLUE[2]-gRED[2])/2)\n",
    "slope = -(gBLUE[1]-gRED[1])/(gBLUE[2]-gRED[2])\n",
    "abline(a=midgg[2]-slope*midgg[1],b=slope)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix : \n",
    "\n",
    "In a two class setting, the confusion matrix (reporting the number of actual class / predicted class) have four entries:\n",
    " \n",
    "|            | Predicted False   | Predicted True  |\n",
    "|:----------:|-------------------|-----------------|\n",
    "|Actual False|  True Negative TN |False Positive FP|\n",
    "|Actual True | False Negative FN |True Positive TP |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification trees : the basics\n",
    "\n",
    "A decision tree (DT) partitions the input space into mutually exclusive regions (the divide-and-conquer approach). Therefore DT are easy to interpret.\n",
    "\n",
    "DT nodes can be classified in internal nodes, determining which child node to visit next, or terminal nodes, partitioning the input space. \n",
    "\n",
    "In classification trees (regression trees also exist), the terminal nodes contain a label that indicates the class for the associated input region. To classify, each new sample is presented to the DT root node of the branch to one of the root’s children. The procedure is repeated recursively until a leaf. The sample is then classified as the label of that leaf.\n",
    "\n",
    "The DT learning procedure has two steps known as *tree growing* and *tree pruning*.\n",
    "\n",
    "* During *tree growing*, an iterative, exhaustive, search is performed to find the successive splits that best reduces a certain cost function.\n",
    "\n",
    "    * The gini impurity (used in CART) : how often a randomly chosen element would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. It is minimum (zero) for class-homogeneous nodes.\n",
    "    \n",
    "    \\begin{equation}\n",
    "         Gini = \\sum_{i=1}^C p_i * (1-p_i) = 1-\\sum_{i=1}^C p_i^2\\\\\n",
    "    \\end{equation}\n",
    "\n",
    "    * The information gain (IG, used in ID3,C4.5) : based on the concept of entropy and information content from information theory. Basically the IG is the entropy of the parent node minus the weighted sum of the children node entropy.\n",
    "\n",
    "    \\begin{equation}\n",
    "         IG = -\\sum_{i=1}^C p_i * log_2 (p_i) - \\sum_{a} p(a) \\sum_{i=1}^C Pr(i|a) * log_2 (Pr(i|a))  \\\\\n",
    "    \\end{equation}\n",
    "   \n",
    "\n",
    "* Pruning uses a complexity based measure of the tree performance to avoid overfitting.\n",
    "\n",
    "\n",
    "Animated version of decision trees : http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset : spam prior, available features,...\n",
    "\n",
    "You first have to explore the dataset before going to classification. This step can be long but is actually very important. In particular, obtain or observe the following interesting elements :\n",
    "* Number of rows and columns\n",
    "* Is there any missing values ?\n",
    "* Spam prior\n",
    "* Name and basic statistics for each variables\n",
    "* Histogram (or other relevant plot) per class (spam vs non-spam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification : classifier, performance metrics, train set and test set\n",
    "\n",
    "The goal of a classification task is to automatically assign data to predefined discrete\n",
    "classes (here spam vs non-spam). The obtained model is often called a classifier. \n",
    "\n",
    "The most used performance metric for classification is the misclassification rate. In this notebook, we will use it but other performance metrics exist. Of course, each performance metric has its pros and cons.\n",
    "\n",
    "The goal of supervised learning is to predict useful information. Therefore we must divide our dataset in two part :\n",
    "\n",
    "* *train set* : From which we train our model, assuming we know the class labels\n",
    "* *test set* : On which we test our model\n",
    "\n",
    "Sometimes, when we want to validate parameters, a *validation set* is also used.\n",
    "\n",
    "This train/test partition can be very simple (one sample in train set, one in test set, repeat) or more elaborate (10-fold cross-validation).\n",
    "\n",
    "First, obtain the performance of a DT on the spambase dataset using a very simple partition. Also check the importance of each feature using the obtained DT. You can use packages *rpart* or *tree*.\n",
    "\n",
    "Then plot the obtained tree using packages *rpart.plot*.\n",
    "\n",
    "Finally, implement a 10-fold cross-validation for assessing your DT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning : random forest\n",
    "\n",
    "A random forest (RF) consists in an ensemble of multiple decision trees trained at the same time and predicting the class as the mode of the single-tree prediction.\n",
    "It combines the *bagging* idea and random selection of features, which is different for each tree. Compared to DT, RF to overfit less and reduce the variance of the estimation. Here is an implementation helper (of course, multiple variants exist) :\n",
    "\n",
    "* The number of trees is a compromise between training time and performance.\n",
    "* Each tree is built on a subset of features, usually the squared root of the total number of features, drawn randomly.\n",
    "* For a new sample, the predicted class is simply the mean of predicted probabilities on all trained trees.\n",
    "\n",
    "Implement a random forest with five trees from your (simple data partition) DT. Try with 8 and 20 features per tree. Do not use the *randomForest* package.\n",
    "\n",
    "Then, implement a 10-fold cross-validation for assessing your DT. Plot the performances according to the number of trained trees. At that point you can use the  *randomForest* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning : Boosting\n",
    "\n",
    "Boosting produces a serie of classifiers (here $T$ trees with $1 \\leq t \\leq T$). The training set used for each classifier (here tree) of the serie is chosen based on the performance of the earlier classifier(s) in the serie. Unlike bagging (like RF), the different classifier cannot be trained in parallel as they depend of the previous one(s). Here is an implementation helper (multiple variants exist) :\n",
    "\n",
    "* The number of trees is a compromise between training time and performance. Here we will train 15 trees.\n",
    "* Starting probability of picking each sample $i$ (with $1 \\leq i \\leq N$) is 1/N (the number of samples in train set). After the first tree, it becomes simply proportional to the weight $w_i$.\n",
    "* The misclassification metric ($misc$) on the train set is also linearly weighted by $w_i$. We will call it $wmisc$.\n",
    "* It allows to compute $\\alpha_t = log((1-wmisc[t])/wmisc[t])$. It represents a confidence on that tree (or classifier).\n",
    "* Then $\\alpha_t$ allows to update $w_i = w_i*exp(alpha_t*misc_t)$.\n",
    "* Finally, the predicted values are the average of all trees, linearly weighted by $\\alpha_t$.\n",
    "* The trees are usually depth-forced : add *control=tree.control(Ntrain,mincut=10)* as a parameter to *tree()*.\n",
    "\n",
    "For this exercise, we will use the *tree* package. It is therefore easier to recode the spam labels as -1/+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsample <- nrow(spambase)\n",
    "data(spambase)\n",
    "spambase$is_spam = (2*as.integer(spambase$is_spam)-3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare, in terms of misclassifcation, a regular tree against a boosted 15-trees model. You can re-use the code of the ''very simple partition'' as for the first tree we designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
